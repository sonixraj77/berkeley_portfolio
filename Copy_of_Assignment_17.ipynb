{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sonixraj77/berkeley_portfolio/blob/main/Copy_of_Assignment_17.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WSchguD4hakM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "a50f71fd-79eb-4db3-89c2-c7d4dc120071"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not fetch dataset using ucimlrepo. Falling back to local CSV file.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'bank-additional-full.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-dc74b965c288>\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mucimlrepo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_ucirepo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mbank_marketing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_ucirepo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m222\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Bank Marketing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ucimlrepo'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-dc74b965c288>\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not fetch dataset using ucimlrepo. Falling back to local CSV file.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bank-additional-full.csv\"\u001b[0m  \u001b[0;31m# Update path if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bank-additional-full.csv'"
          ]
        }
      ],
      "source": [
        "###########################################\n",
        "# BANK MARKETING CLASSIFICATION PROJECT\n",
        "###########################################\n",
        "\n",
        "# 1. OVERVIEW & BUSINESS UNDERSTANDING\n",
        "\"\"\"\n",
        "Goal:\n",
        "Predict whether a client will subscribe to a term deposit based on various demographic,\n",
        "economic, and campaign-related features.\n",
        "\n",
        "Why It Matters:\n",
        "- Targeting likely subscribers reduces marketing costs.\n",
        "- Increases conversion rates and improves ROI on telemarketing campaigns.\n",
        "\n",
        "Dataset:\n",
        "- Sourced from UCI Machine Learning Repository (Portuguese bank marketing dataset).\n",
        "- Features: age, job, marital status, education, default, housing, loan status, contact method,\n",
        "  month/day_of_week, duration, campaign, pdays, previous, poutcome, and economic indicators.\n",
        "- Target variable: 'y' (yes/no to subscribing).\n",
        "\n",
        "Deliverable:\n",
        "- Compare KNN, Logistic Regression, Decision Tree, and SVM (with a subsampled or simpler approach).\n",
        "- Present descriptive stats, inference, and actionable findings.\n",
        "- Outline next steps for further improvements.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 2. IMPORT LIBRARIES\n",
        "# ============================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC  # We'll show how to sub-sample for the RBF SVM\n",
        "#from sklearn.svm import LinearSVC  # (Optional) If you'd like a faster linear SVM\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 3. LOAD / FETCH DATA\n",
        "# ============================\n",
        "\"\"\"\n",
        "You can either:\n",
        "1) Use the ucimlrepo library to fetch the dataset directly (if environment allows),\n",
        "2) Or read from a local CSV file like 'bank-additional-full.csv'.\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    from ucimlrepo import fetch_ucirepo\n",
        "    bank_marketing = fetch_ucirepo(id=222)  # Bank Marketing\n",
        "    X_raw = bank_marketing.data.features\n",
        "    y_raw = bank_marketing.data.targets\n",
        "    df = pd.concat([X_raw, y_raw], axis=1)\n",
        "    # Rename target to 'y' if not already\n",
        "    if 'y' not in df.columns:\n",
        "        df.columns = list(X_raw.columns) + ['y']\n",
        "except:\n",
        "    print(\"Could not fetch dataset using ucimlrepo. Falling back to local CSV file.\")\n",
        "    data_path = \"bank-additional-full.csv\"  # Update path if needed\n",
        "    df = pd.read_csv(data_path, sep=\";\")\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 4. DATA INSPECTION\n",
        "# ============================\n",
        "print(\"Data Loaded Successfully!\")\n",
        "print(\"DataFrame Shape:\", df.shape)\n",
        "display(df.head())\n",
        "\n",
        "print(\"\\n--- Data Info ---\")\n",
        "df.info()\n",
        "\n",
        "print(\"\\n--- Descriptive Statistics (Numeric) ---\")\n",
        "display(df.describe())\n",
        "\n",
        "print(\"\\n--- Target Variable Distribution ---\")\n",
        "print(df['y'].value_counts())\n",
        "\n",
        "# Quick plot of target distribution\n",
        "plt.figure(figsize=(4,4))\n",
        "df['y'].value_counts().plot(kind='bar', color=['lightgreen','salmon'], edgecolor='black')\n",
        "plt.title(\"Subscription (y) Distribution\")\n",
        "plt.xlabel(\"Term Deposit Subscription (no / yes)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 5. DATA PREPARATION\n",
        "# ============================\n",
        "print(\"\\n--- Checking for Missing Values ---\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Convert Target (yes/no) to Binary (1/0)\n",
        "if df['y'].dtype == 'object':\n",
        "    df['y'] = df['y'].map({'yes':1, 'no':0})\n",
        "\n",
        "# Identify categorical vs numeric columns\n",
        "cat_cols = df.select_dtypes(include=['object']).columns\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns.drop('y', errors='ignore')\n",
        "\n",
        "print(\"\\nCategorical Columns:\", cat_cols.tolist())\n",
        "print(\"Numeric Columns:\", num_cols.tolist())\n",
        "\n",
        "# One-hot encode categorical features\n",
        "df_encoded = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
        "\n",
        "# Separate features & target\n",
        "X = df_encoded.drop('y', axis=1)\n",
        "y = df_encoded['y']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "print(\"\\nTrain shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
        "\n",
        "# Scale numeric columns (especially helpful for KNN and SVM)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = X_train.copy()\n",
        "X_test_scaled = X_test.copy()\n",
        "\n",
        "# We'll scale columns that were originally numeric (some columns might be dummy-coded)\n",
        "encoded_num_cols = [c for c in X_train.columns if c in num_cols]\n",
        "X_train_scaled[encoded_num_cols] = scaler.fit_transform(X_train_scaled[encoded_num_cols])\n",
        "X_test_scaled[encoded_num_cols] = scaler.transform(X_test_scaled[encoded_num_cols])\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 6. MODELING\n",
        "# ============================\n",
        "\n",
        "def evaluate_model(model, X_test_data, y_test_data, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Prints out Accuracy, Precision, Recall, F1-score, ROC-AUC,\n",
        "    Confusion Matrix, and Classification Report.\n",
        "    \"\"\"\n",
        "    y_pred = model.predict(X_test_data)\n",
        "    acc = accuracy_score(y_test_data, y_pred)\n",
        "    prec = precision_score(y_test_data, y_pred)\n",
        "    rec = recall_score(y_test_data, y_pred)\n",
        "    f1 = f1_score(y_test_data, y_pred)\n",
        "\n",
        "    # Compute AUC if possible\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_proba = model.predict_proba(X_test_data)[:, 1]\n",
        "    elif hasattr(model, \"decision_function\"):\n",
        "        y_proba = model.decision_function(X_test_data)\n",
        "    else:\n",
        "        y_proba = None\n",
        "\n",
        "    auc_score = roc_auc_score(y_test_data, y_proba) if y_proba is not None else float('nan')\n",
        "\n",
        "    print(f\"\\n--- {model_name} Evaluation ---\")\n",
        "    print(f\"Accuracy : {acc:.3f}\")\n",
        "    print(f\"Precision: {prec:.3f}\")\n",
        "    print(f\"Recall   : {rec:.3f}\")\n",
        "    print(f\"F1-Score : {f1:.3f}\")\n",
        "    print(f\"ROC-AUC  : {auc_score:.3f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test_data, y_pred))\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test_data, y_pred))\n",
        "\n",
        "\n",
        "# 6.1 KNN\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "evaluate_model(knn, X_test_scaled, y_test, \"KNN\")\n",
        "\n",
        "\n",
        "# 6.2 Logistic Regression\n",
        "logreg = LogisticRegression(solver='liblinear', random_state=42)\n",
        "logreg.fit(X_train_scaled, y_train)\n",
        "evaluate_model(logreg, X_test_scaled, y_test, \"Logistic Regression\")\n",
        "\n",
        "\n",
        "# 6.3 Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42, max_depth=5)\n",
        "dt.fit(X_train, y_train)  # Decision Trees do fine with unscaled data\n",
        "evaluate_model(dt, X_test, y_test, \"Decision Tree\")\n",
        "\n",
        "\n",
        "# 6.4 SVM (RBF) with SUBSAMPLING to avoid long run-times\n",
        "print(\"\\nSubsampling the training set for SVM to reduce training time...\")\n",
        "sample_frac = 0.3  # e.g. 30% of the training data\n",
        "X_train_svm = X_train_scaled.sample(frac=sample_frac, random_state=42)\n",
        "y_train_svm = y_train.loc[X_train_svm.index]\n",
        "\n",
        "svm_clf = SVC(\n",
        "    kernel='rbf',\n",
        "    probability=True,\n",
        "    random_state=42,\n",
        "    C=1.0,\n",
        "    gamma='scale',\n",
        "    max_iter=10000  # limiting iterations can also help\n",
        ")\n",
        "svm_clf.fit(X_train_svm, y_train_svm)\n",
        "evaluate_model(svm_clf, X_test_scaled, y_test, \"SVM (RBF) - Subsampled\")\n",
        "\n",
        "\n",
        "# OPTIONAL: Instead of RBF with subsampling, try a linear SVC approach\n",
        "# This is much faster for large datasets, but won't produce probabilities\n",
        "# unless you wrap with CalibratedClassifierCV.\n",
        "\"\"\"\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "print(\"\\n(OPTION) Using LinearSVC instead of RBF SVM...\")\n",
        "linear_svc = LinearSVC(random_state=42, max_iter=10000)\n",
        "linear_svc_calibrated = CalibratedClassifierCV(linear_svc, cv=5)\n",
        "linear_svc_calibrated.fit(X_train_scaled, y_train)\n",
        "\n",
        "evaluate_model(linear_svc_calibrated, X_test_scaled, y_test, \"Linear SVC (Calibrated)\")\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 7. MODEL COMPARISON\n",
        "# ============================\n",
        "models_dict = {\n",
        "    \"KNN\": knn,\n",
        "    \"Logistic Regression\": logreg,\n",
        "    \"Decision Tree\": dt,\n",
        "    \"SVM (RBF) - Subsampled\": svm_clf\n",
        "    # \"Linear SVC (Calibrated)\": linear_svc_calibrated (if you enable it)\n",
        "}\n",
        "\n",
        "results = []\n",
        "for m_name, m in models_dict.items():\n",
        "    # For KNN/LogReg/SVM we use the scaled test set\n",
        "    if \"Tree\" not in m_name:\n",
        "        X_test_eval = X_test_scaled\n",
        "    else:\n",
        "        X_test_eval = X_test\n",
        "\n",
        "    y_pred = m.predict(X_test_eval)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred)\n",
        "    rec = recall_score(y_test, y_pred)\n",
        "    f1_val = f1_score(y_test, y_pred)\n",
        "\n",
        "    if hasattr(m, \"predict_proba\"):\n",
        "        y_proba = m.predict_proba(X_test_eval)[:, 1]\n",
        "    elif hasattr(m, \"decision_function\"):\n",
        "        y_proba = m.decision_function(X_test_eval)\n",
        "    else:\n",
        "        y_proba = None\n",
        "\n",
        "    auc = roc_auc_score(y_test, y_proba) if y_proba is not None else np.nan\n",
        "\n",
        "    results.append([m_name, acc, prec, rec, f1_val, auc])\n",
        "\n",
        "comparison_df = pd.DataFrame(\n",
        "    results,\n",
        "    columns=[\"Model\",\"Accuracy\",\"Precision\",\"Recall\",\"F1-Score\",\"ROC-AUC\"]\n",
        ").sort_values(by=\"Accuracy\", ascending=False)\n",
        "\n",
        "print(\"\\n=== Model Performance Comparison ===\")\n",
        "display(comparison_df)\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 8. FINDINGS & INSIGHTS\n",
        "# ============================\n",
        "\"\"\"\n",
        "1) Interpret Model Results:\n",
        "   - Check which model has highest accuracy, or if recall/precision is more important for the business.\n",
        "   - SVM might do well, but keep in mind we only used ~30% of training data.\n",
        "\n",
        "2) Impact on the Bank:\n",
        "   - A high-recall model finds more potential subscribers but may annoy uninterested clients.\n",
        "   - A high-precision model saves marketing calls but might miss some potential clients.\n",
        "\n",
        "3) Class Imbalance:\n",
        "   - The majority class is 'no', which can lead to lower recall for the minority class ('yes').\n",
        "   - Techniques like SMOTE or adjusting class_weights could help improve minority (1) recall.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 9. NEXT STEPS & RECOMMENDATIONS\n",
        "# ============================\n",
        "\"\"\"\n",
        "- Hyperparameter Tuning:\n",
        "  Use GridSearchCV/RandomizedSearchCV for each model, including the 'C' and 'gamma' for SVM.\n",
        "- Feature Engineering:\n",
        "  Possibly combine or transform certain features, use domain expertise to create new features.\n",
        "- Handle Imbalance:\n",
        "  Try oversampling (SMOTE) or set class_weight='balanced' in some models to improve minority class performance.\n",
        "- Alternate Algorithms:\n",
        "  Try ensemble methods (RandomForest, XGBoost) which can yield strong results with large datasets.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nNotebook Complete. Review the 'Model Performance Comparison' table for final results!\")\n"
      ]
    }
  ]
}